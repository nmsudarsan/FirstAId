
# ğŸ©º AI First Aid Instructor (Voice + Vision)

# Please go through the AI_FirstAid_Project_Report.pdf file to view input and output, along with the workflow of the project.

This project is an AI-powered **First Aid Instructor** that uses voice input, image analysis, and LLMs to provide actionable, step-by-step medical assistance with **visual illustrations**.

Built using:
- ğŸ”Š **Voice input** via `SpeechRecognition` + Whisper (Groq)
- ğŸ§  **First aid recommendation** via Groq LLM (`llama3-8b`)
- ğŸ–¼ï¸ **Image generation** via Hugging Face FLUX model
- ğŸ—£ï¸ **Text-to-speech** via `gTTS` and `ElevenLabs`
- ğŸŒ Interactive frontend built with **Gradio**

---

## ğŸš€ Features

- ğŸ™ï¸ Record patient voice using microphone
- ğŸ¤– Transcribes voice to text using Whisper model via Groq
- ğŸ§  Analyzes user speech + image using multimodal LLM (Groq)
- ğŸ–¼ï¸ Extracts visual first aid steps and generates step-wise illustrations using Hugging Face
- ğŸ—£ï¸ Responds with spoken instructions using ElevenLabs/gTTS
- ğŸ–¼ï¸ Scrollable image gallery for visual aid

---

## ğŸ§± Tech Stack

| Component            | Library / API                      |
|----------------------|------------------------------------|
| LLM (Text/Image)     | `groq` (LLaMA 3)                    |
| Audio Input          | `speechrecognition`, `pyaudio`     |
| Audio Output         | `gtts`, `elevenlabs`               |
| Audio Handling       | `pydub`, `ffmpeg`                  |
| Image Generation     | Hugging Face FLUX (FLUX.1-dev)     |
| Image Display        | `gradio`                           |
| Prompt Parsing       | `llama3-8b-8192` via Groq API      |

---

## ğŸ“¦ Setup Instructions

1. **Clone the repo:**
```bash
git clone https://github.com/nmsudarsan/FirstAId.git
cd FirstAId
```

2. **Create virtual environment:**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies:**
```bash
pip install -r requirements.txt
```

4. **Set up your `.env` file:**
Create a `.env` file in the root directory and add:

```
GROQ_API_KEY=your_groq_api_key
HF_TOKEN=your_huggingface_token
ELEVENLABS_API_KEY=your_elevenlabs_api_key
```

5. **Run the app:**
```bash
python gradio_app.py
```

---

## ğŸ“ Directory Structure

```
â”œâ”€â”€ gradio_app.py              # Main Gradio interface
â”œâ”€â”€ voice_of_the_patient.py    # Voice recording + transcription
â”œâ”€â”€ voice_of_the_doctor.py     # TTS voice response
â”œâ”€â”€ firstaid_brain.py          # Multimodal analysis using Groq
â”œâ”€â”€ step_extractor.py          # Extracts + simplifies visual steps
â”œâ”€â”€ image_generator.py         # Uses HF FLUX to generate step images
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env (not committed)       # Your API keys
```

---

## ğŸ¨ Sample Output

Please go through AI_FirstAid_Project_Report.pdf file to know more.

---

## ğŸ›¡ï¸ Disclaimers

- This is **not a replacement for professional medical advice**.
- Intended for educational and demonstration purposes only.

---

## ğŸ¤ Contributing

Pull requests welcome. For major changes, please open an issue first.

---

## ğŸ“œ License

This project is licensed under the MIT License.
